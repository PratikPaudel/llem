import pinecone
from openai import OpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
import pandas as pd
from tqdm import tqdm
import os
from google.colab import userdata

# Get secrets from Colab
OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')
PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')

# Initialize Pinecone with new method
pc = pinecone.Pinecone(
    api_key=PINECONE_API_KEY
)

# Get your index
index = pc.Index("langchainvector")

# Initialize OpenAI
openai_client = OpenAI(api_key=OPENAI_API_KEY)
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

def process_entries(csv_path, start_idx=0, end_idx=30):
    """Process entries from start_idx to end_idx from the CSV file."""
    # Read CSV file
    print(f"Reading CSV from {csv_path}")
    df = pd.read_csv(csv_path)

    # Validate indices
    end_idx = min(end_idx, len(df) - 1)
    print(f"Processing entries from index {start_idx} to {end_idx}")

    # Initialize counters
    successful = 0
    failed = 0
    vectors = []

    # Process specified range of entries
    for idx in tqdm(range(start_idx, end_idx + 1), desc="Processing entries"):
        try:
            row = df.iloc[idx]

            # Create embedding
            response = openai_client.embeddings.create(
                model="text-embedding-ada-002",
                input=row['Content']
            )
            embedding = response.data[0].embedding

            # Prepare metadata
            metadata = {
                'text': row['Content'],
                'author': row['Author'],
                'date': row['Date'],
                'source': 'human_experiences_collection'
            }

            # Prepare vector
            vector = {
                'id': f'story_{idx}',
                'values': embedding,
                'metadata': metadata
            }

            vectors.append(vector)

            # Upload in batches of 5
            if len(vectors) >= 5:
                index.upsert(vectors=vectors)
                successful += len(vectors)
                print(f"\nUploaded batch. Processed through index: {idx}")
                vectors = []

        except Exception as e:
            failed += 1
            print(f"\nError processing entry {idx}: {str(e)}")
            continue

    # Upload any remaining vectors
    if vectors:
        try:
            index.upsert(vectors=vectors)
            successful += len(vectors)
        except Exception as e:
            failed += len(vectors)
            print(f"\nError in final batch upload: {str(e)}")

    print("\nBatch Upload Complete!")
    print(f"Successfully uploaded: {successful} entries")
    print(f"Failed uploads: {failed} entries")
    print(f"Total processed: {successful + failed} entries")
    print(f"Processed range: {start_idx} to {end_idx}")
    print(f"\nTo process next batch, use start_idx={end_idx + 1}")

if __name__ == "__main__":
    csv_path = '/content/Processed.csv'
    process_entries(csv_path, start_idx=460, end_idx=508)
